2024-10-12 22:09:01,663 [INFO] model architecture: Ark(
  (encoder): ArkEncoder(
    (word_embedding): Embedding(11528, 256)
    (position_embedding): Embedding(128, 256)
    (channel_embedding): Embedding(3, 256)
    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (fusion_ch): FusionChannel(
      (lin): MultiLinear(
        (dense): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.5, inplace=False)
          (3): LeakyReLU(negative_slope=0.01)
          (4): Linear(in_features=256, out_features=256, bias=True)
          (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (6): Dropout(p=0.5, inplace=False)
          (7): LeakyReLU(negative_slope=0.01)
        )
      )
    )
  )
  (decoder): ArkDecoder(
    (transformer_layers): TransformerLayers(
      (transformer_blocks): ModuleList(
        (0-7): 8 x TransformerLayer(
          (attention): MultiHeadAttention(
            (attention): Attention()
          )
          (ffn): PositionWiseFFN(
            (linear): MultiLinear(
              (dense): Sequential(
                (0): Linear(in_features=256, out_features=256, bias=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LeakyReLU(negative_slope=0.01)
                (3): Linear(in_features=256, out_features=256, bias=True)
                (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (5): LeakyReLU(negative_slope=0.01)
              )
            )
            (add_norm): AddNorm(
              (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0, inplace=False)
            )
          )
          (add_norm1): AddNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (add_norm2): AddNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
  )
  (output_layer): ArkClassifier(
    (fusion): TransformerLayer(
      (attention): MultiHeadAttention(
        (attention): Attention()
      )
      (ffn): PositionWiseFFN(
        (linear): MultiLinear(
          (dense): Sequential(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Linear(in_features=256, out_features=256, bias=True)
            (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (5): LeakyReLU(negative_slope=0.01)
          )
        )
        (add_norm): AddNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (add_norm1): AddNorm(
        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (add_norm2): AddNorm(
        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )
    (classifier): Linear(in_features=256, out_features=2, bias=True)
  )
)
2024-10-12 22:09:01,663 [INFO] optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
2024-10-12 22:09:01,663 [INFO] loss_fn: CrossEntropyLoss()
2024-10-12 22:09:01,663 [INFO] fit on cpu
2024-10-12 22:09:01,664 [INFO] num_params: 4326274
2024-10-12 22:09:01,664 [INFO] num_class: 2
2024-10-12 22:09:01,664 [INFO] epochs: 200
2024-10-12 22:09:01,664 [INFO] stop_loss_value: 0.1
2024-10-12 22:09:01,664 [INFO] stop_min_epoch: 20

2024-10-12 22:09:15,988 [INFO] Epoch 1, Batch (1/58), Loss: 0.7937
2024-10-12 22:09:29,820 [INFO] Epoch 1, Batch (2/58), Loss: 0.7391
2024-10-12 22:09:44,000 [INFO] Epoch 1, Batch (3/58), Loss: 0.7242
