2024-11-08 21:34:31,257 [INFO] model architecture: Ark(
  (encoder): ArkEncoder(
    (word_embedding): Embedding(11528, 256)
    (position_embedding): Embedding(128, 256)
    (channel_embedding): Embedding(3, 256)
    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (decoder): ArkDecoder(
    (transformer_layers1): ModuleList(
      (0-7): 8 x TransformerLayer(
        (attention): MultiHeadAttention(
          (attention): Attention()
        )
        (ffn): PositionWiseFFN(
          (linear): MultiLinear(
            (dense): Sequential(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Linear(in_features=256, out_features=256, bias=True)
              (3): LeakyReLU(negative_slope=0.01)
            )
          )
        )
        (add_norm1): AddNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
        (add_norm2): AddNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (transformer_layers2): ModuleList(
      (0-7): 8 x TransformerLayer(
        (attention): MultiHeadAttention(
          (attention): Attention()
        )
        (ffn): PositionWiseFFN(
          (linear): MultiLinear(
            (dense): Sequential(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Linear(in_features=256, out_features=256, bias=True)
              (3): LeakyReLU(negative_slope=0.01)
            )
          )
        )
        (add_norm1): AddNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
        (add_norm2): AddNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (transformer_layers3): ModuleList(
      (0-7): 8 x TransformerLayer(
        (attention): MultiHeadAttention(
          (attention): Attention()
        )
        (ffn): PositionWiseFFN(
          (linear): MultiLinear(
            (dense): Sequential(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Linear(in_features=256, out_features=256, bias=True)
              (3): LeakyReLU(negative_slope=0.01)
            )
          )
        )
        (add_norm1): AddNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
        (add_norm2): AddNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (dim_reduce): Linear(in_features=768, out_features=256, bias=True)
  )
  (output_layer): ArkClassifier(
    (fusion): TransformerLayer(
      (attention): MultiHeadAttention(
        (attention): Attention()
      )
      (ffn): PositionWiseFFN(
        (linear): MultiLinear(
          (dense): Sequential(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=256, out_features=256, bias=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (add_norm1): AddNorm(
        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (add_norm2): AddNorm(
        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )
    (classifier): Linear(in_features=256, out_features=2, bias=True)
  )
)
2024-11-08 21:34:31,257 [INFO] optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
2024-11-08 21:34:31,272 [INFO] loss_fn: CrossEntropyLoss()
2024-11-08 21:34:31,272 [INFO] fit on cpu
2024-11-08 21:34:31,274 [INFO] num_params: 6498050
2024-11-08 21:34:31,274 [INFO] num_class: 2
2024-11-08 21:34:31,275 [INFO] epochs: 200
2024-11-08 21:34:31,275 [INFO] stop_loss_value: 0.1
2024-11-08 21:34:31,275 [INFO] stop_min_epoch: 20

