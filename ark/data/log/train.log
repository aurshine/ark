[ 2024-09-19 18:48:58]	fit on cpu
[ 2024-09-19 18:48:58]	model architecture: Ark(
  (encoder): ArkEncoder(
    (word_embedding): Embedding(21458, 64)
    (position_embedding): Embedding(128, 64)
    (channel_embedding): Embedding(3, 64)
    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (fusion_ch): FusionChannel(
      (lin): MultiLinear(
        (dense): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.5, inplace=False)
          (3): LeakyReLU(negative_slope=0.01)
          (4): Linear(in_features=64, out_features=64, bias=True)
          (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (6): Dropout(p=0.5, inplace=False)
          (7): LeakyReLU(negative_slope=0.01)
        )
      )
    )
  )
  (decoder): ArkDecoder(
    (transformer_layers): TransformerLayers(
      (transformer_blocks): ModuleList(
        (0-7): 8 x TransformerLayer(
          (attention): MultiHeadAttention(
            (attention): Attention()
          )
          (ffn): PositionWiseFFN(
            (linear): MultiLinear(
              (dense): Sequential(
                (0): Linear(in_features=64, out_features=64, bias=True)
                (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (2): LeakyReLU(negative_slope=0.01)
                (3): Linear(in_features=64, out_features=64, bias=True)
                (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (5): LeakyReLU(negative_slope=0.01)
              )
            )
            (add_norm): AddNorm(
              (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0, inplace=False)
            )
          )
          (add_norm1): AddNorm(
            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (add_norm2): AddNorm(
            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
  )
  (output_layer): ArkClassifier(
    (fusion): TransformerLayer(
      (attention): MultiHeadAttention(
        (attention): Attention()
      )
      (ffn): PositionWiseFFN(
        (linear): MultiLinear(
          (dense): Sequential(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Linear(in_features=64, out_features=64, bias=True)
            (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (5): LeakyReLU(negative_slope=0.01)
          )
        )
        (add_norm): AddNorm(
          (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (add_norm1): AddNorm(
        (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (add_norm2): AddNorm(
        (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )
    (classifier): Linear(in_features=64, out_features=2, bias=True)
  )
)
[ 2024-09-19 18:48:58]	optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
[ 2024-09-19 18:48:58]	loss_fn: CrossEntropyLoss()
[ 2024-09-19 18:48:58]	num_class: 2
[ 2024-09-19 18:48:58]	epochs: 200
[ 2024-09-19 18:48:58]	stop_loss_value: 0.1
[ 2024-09-19 18:48:58]	stop_min_epoch: 0
[ 2024-09-19 18:49:00]	loss at epoch 2, loss: 0.06834367290139198
[ 2024-09-19 18:49:00]	MetricsAtEpoch: 2	Accuracy: 1.0	Precision: 1.0	Recall: 1.0	F1-score: 1.0
